{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PROJECT - DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect, detect_langs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ast\n",
    "import re\n",
    "import itertools\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset with all the songs lyrics to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/songs_ed_sheeran_elton_john.csv\")\n",
    "print(\"Total songs:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all songs that haven't been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df['lyrics'] != 'Pending'].copy()\n",
    "print(\"Processed songs:\", len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop column with artist_2 as is not giving much information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean['main_artist'] = df_clean['artists'].apply(lambda x: [n.strip() for n in ast.literal_eval(x)][0])\n",
    "df_clean.drop(columns='artist_2', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with None values (should only be in Lyrics for the songs that we haven't found them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.dropna(inplace=True)\n",
    "print(\"Songs with lyrics:\", len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(lyrics):\n",
    "    \"\"\"\n",
    "    This function cleans the lyrics and leave them as a succession of words.\n",
    "    parameters:\n",
    "    lyrics: original string coming from Genius\n",
    "    \"\"\"\n",
    "    lyrics = lyrics.replace(\"\\n\",\" \")\n",
    "    lyrics = lyrics.lower()\n",
    "    lyrics = lyrics.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    \n",
    "    #remove the weblinks\n",
    "    lyrics = re.sub(r'https?:\\/\\/.\\S+', \"\", lyrics)\n",
    "    \n",
    "    #replace the contractions\n",
    "    apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\", \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"} \n",
    "    for key,value in apos_dict.items(): \n",
    "        if key in lyrics: \n",
    "            lyrics = lyrics.replace(key,value) \n",
    "    lyrics = lyrics.strip()\n",
    "    \n",
    "    #One letter in a word should not be present more than twice in continuation \n",
    "    lyrics = ''.join(''.join(s)[:2] for _, s in itertools.groupby(lyrics)) \n",
    "    \n",
    "    #spell check \n",
    "    spell = Speller(lang='en')\n",
    "    lyrics = spell(lyrics)\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['lyrics_cleaned'] = df_clean['lyrics'].apply(clean_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect language of the lyrics and we keep only English songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['language'] = df_clean['lyrics'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df_clean[df_clean['language'] == 'en'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all words for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(lyrics):\n",
    "    tokens = word_tokenize(lyrics)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    lowerwords = [word.lower() for word in words]\n",
    "    clean_words = [w for w in lowerwords if not w in stop_words]\n",
    "    fdist = FreqDist(clean_words)\n",
    "    list_most_used = fdist.most_common()\n",
    "    return { i[0] : i[1] for i in list_most_used }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english['most_used'] = df_english['lyrics'].apply(get_most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lyrics Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english['lyrics_polarity'] = df_english['lyrics'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='valence', y='lyrics_polarity', data=df_english, scatter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split years by decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english['decade'] = pd.cut(x=df_english['year'], bins=[1949, 1959, 1969, 1979, 1989, 1999, 2009, 2029], labels=['50s', '60s', '70s', '80s', '90s', '00s', '10s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.groupby('decade').lyrics_polarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.groupby('decade').popularity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english.to_csv('../data/songs_partial_english.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
